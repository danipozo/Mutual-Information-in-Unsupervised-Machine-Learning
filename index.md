1. Probabilidad
  1.1. Conceptos Básicos
  1.2. Distribuciones
  1.3. Cadenas de Markov

2. Teoría de la Información.
  2.1. Entropía, Información Mutua.
  2.2. Estimadores de I.M.
  2.3. Acotación de I.M.

3. Aprendizaje automático no supervisado
  2.2. Historia
  2.2. Representaciones mediante información Mutua
  2.3. Eliminando el Back Propagation

4. Experimentación


Bibliography:

- https://www.inference.org.uk/itprnn/book.pdf Basic concepts book
- http://mlg.eng.cam.ac.uk/zoubin/course03/
- https://arxiv.org/pdf/1808.06670.pdf Learning deep representations by mutual information estimation and maximization
- https://arxiv.org/pdf/1905.11786.pdf Putting an End to End-to-End
- https://loewex.github.io/Thesis_SindyLoewe_GreedyInfoMax.pdf
- https://arxiv.org/pdf/1807.03748.pdf Contrastive predictive coding.
- http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.707.5001&rep=rep1&type=pdf Paper with definitions of information theory
- https://statweb.stanford.edu/~adembo/stat-310b/lnotes.pdf stanford probability theory



Concepts:
- Back Propagation
- Artificial neural network
- Supervised/unsupervised machine learning
- Random Variable
-





2. Information theory

2.1. Entropy. Mutual Information.

2.1.1. Introduction

Here, I should talk a little bit of why Mutual Information is important. A nice introduction can be found at:https://arxiv.org/pdf/1807.03748.pdf Contrastive predictive coding

Text could continue as follows:

T
